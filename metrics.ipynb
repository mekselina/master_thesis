{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDaVMb7gJ-UK"
      },
      "source": [
        "# Overlap metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDQOAjT4rHOM"
      },
      "source": [
        "This section contains the evaluation of 5sequences generations of large and small models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEutWnhiWRAq",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "#!pip install transformers==3.3.1\n",
        "!pip install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets==1.0.2\n",
        "#!pip install datasets==1.5.0\n",
        "\n",
        "#clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_iZmfwH36Fc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "c =  '#7eca9c'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aWoa3AtDXN0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from itertools import product\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "import transformers\n",
        "import datasets\n",
        "from transformers import AutoTokenizer, TFT5ForConditionalGeneration\n",
        "import datetime\n",
        "import os\n",
        "import pandas\n",
        "import warnings\n",
        "from functools import reduce\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ5CoTk-DXN6"
      },
      "outputs": [],
      "source": [
        "tf_version = tf.__version__\n",
        "print(\"Tensorflow: \", tf_version)\n",
        "print(\"Transformers: \", transformers.__version__)\n",
        "print(\"Datasets: \", datasets.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5wQMUDPjveO"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score\n",
        "!pip install bleu\n",
        "\n",
        "from datasets import load_metric\n",
        "rouge = load_metric(\"rouge\", seed=0)\n",
        "bleu = load_metric(\"bleu\", seed = 0)\n",
        "import string\n",
        "\n",
        "#things necessary to compute bleurt\n",
        "import sys\n",
        "sys.argv = sys.argv[:1]\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvsYPsSlO_R6"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QMZ2qTNpNVj"
      },
      "outputs": [],
      "source": [
        "#row wise\n",
        "def evaluator(somedata, metric): #takes data\n",
        "  \"\"\"\n",
        "  input\n",
        "    somedata: input data, column names should be of the type MODEL_decodingnr (i.e. DIALO_kp0, GPT2_tk0,...)\n",
        "    can take also a dataset with different models generations at once\n",
        "    metric: a string, either 'rouge', 'bleu-1', 'bleu-3' or 'bleu-4'\n",
        "\n",
        "  output\n",
        "    the input dataframe is extended with one column per generation containing the related rouge/bleu score\n",
        "  \"\"\"\n",
        "  data = somedata.copy()\n",
        "  data = data.fillna('')\n",
        "  tf.random.set_seed(0)\n",
        "  cols = [el for el in list(data.columns) if el not in ['index','HS','reference_CN', 'decoding']]\n",
        "  diz = dict((el,[]) for el in cols)\n",
        "  diz['index'] = []\n",
        "\n",
        "  if metric == 'rouge':\n",
        "    rouge = load_metric(\"rouge\", seed=0)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "      diz['index'].append(data.loc[i,'index'])\n",
        "      for c in cols:\n",
        "        rouge.add(prediction = data[c][i], reference = data['reference_CN'][i])\n",
        "        rouge_output = rouge.compute(rouge_types=['rougeL'], use_agregator=False)\n",
        "        diz[c].append(rouge_output['rougeL'][0][2]) #fmeasure\n",
        "\n",
        "  if metric == 'bleu-1':\n",
        "    bleu = load_metric(\"bleu\", seed = 0)\n",
        "    diz['index'] = data['index']\n",
        "    for c in cols:\n",
        "      bleu_tokspred = [x.split() for x in data[c].to_list()]\n",
        "      bleu_preds= [[x.translate(str.maketrans('', '', string.punctuation)).lower() for x in sublist] for sublist in bleu_tokspred]\n",
        "      bleu_toksref=[x.split() for x in data['reference_CN'].to_list()]\n",
        "      bleu_refs= [[[x.translate(str.maketrans('', '', string.punctuation)).lower() for x in sublist]] for sublist in bleu_toksref]\n",
        "      for i in range(len(data)):\n",
        "        if len(bleu_preds[i]) == 0:\n",
        "          diz[c].append(0)\n",
        "        else:\n",
        "          bleu.add(prediction=bleu_preds[i], reference=bleu_refs[i])\n",
        "          bleu_output = bleu.compute(max_order=1)\n",
        "          diz[c].append(bleu_output['bleu'])\n",
        "\n",
        "  if metric == 'bleu-3':\n",
        "    diz['index'] = data['index']\n",
        "    bleu = load_metric(\"bleu\", seed = 0)\n",
        "    for c in cols:\n",
        "      bleu_tokspred = [x.split() for x in data[c].to_list()]\n",
        "      bleu_preds= [[x.translate(str.maketrans('', '', string.punctuation)).lower() for x in sublist] for sublist in bleu_tokspred]\n",
        "      bleu_toksref=[x.split() for x in data['reference_CN'].to_list()]\n",
        "      bleu_refs= [[[x.translate(str.maketrans('', '', string.punctuation)).lower() for x in sublist]] for sublist in bleu_toksref]\n",
        "\n",
        "      for i in range(len(data)):\n",
        "        if len(bleu_preds[i]) == 0:\n",
        "          diz[c].append(0)\n",
        "        else:\n",
        "          bleu.add(prediction=bleu_preds[i], reference=bleu_refs[i])\n",
        "          bleu_output = bleu.compute(max_order=3)\n",
        "          diz[c].append(bleu_output['bleu'])\n",
        "\n",
        "  if metric == 'bleu-4':\n",
        "    diz['index'] = data['index']\n",
        "    bleu = load_metric(\"bleu\", seed = 0)\n",
        "    for c in cols:\n",
        "      bleu_tokspred = [x.split() for x in data[c].to_list()]\n",
        "      bleu_preds= [[x.translate(str.maketrans('', '', string.punctuation)).lower() for x in sublist] for sublist in bleu_tokspred]\n",
        "      bleu_toksref=[x.split() for x in data['reference_CN'].to_list()]\n",
        "      bleu_refs= [[[x.translate(str.maketrans('', '', string.punctuation)).lower() for x in sublist]] for sublist in bleu_toksref]\n",
        "\n",
        "      for i in range(len(data)):\n",
        "        if len(bleu_preds[i]) == 0:\n",
        "          diz[c].append(0)\n",
        "        else:\n",
        "          bleu.add(prediction=bleu_preds[i], reference=bleu_refs[i])\n",
        "          bleu_output = bleu.compute(max_order=4)\n",
        "          diz[c].append(bleu_output['bleu'])\n",
        "\n",
        "  out_data = pandas.DataFrame(diz)\n",
        "  out_data.columns = [metric+'_'+el if el!='index' else el for el in list(out_data.columns)]\n",
        "  return data.merge(out_data, on='index')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8UCCqAYgEud"
      },
      "outputs": [],
      "source": [
        "def selecter(evaluated_data, metric, by):\n",
        "  \"\"\"\n",
        "  input\n",
        "    evaluated_data: data evaluated by evaluator\n",
        "    metric: a string, for the moment only rouge implemented\n",
        "    by: select the generation with highest score among 'all' rows, 'model', 'decoding', 'model-decoding'\n",
        "\n",
        "  output\n",
        "    dataframe containing the selected columns and related scores\n",
        "  \"\"\"\n",
        "\n",
        "  data = evaluated_data.copy()\n",
        "  #keep only numeric columns with scores\n",
        "  filter_col = [col for col in data.columns if col.startswith(metric)]\n",
        "  dataf = data[filter_col]\n",
        "  out_data =  data[['index', 'HS', 'generated_CN', 'reference_CN', 'decoding']]\n",
        "\n",
        "  #the highest generation is chosen for each model-decoding (e.g. highest T5_tk, highest DIALO_tp, ecc)\n",
        "  if by == 'model_decoding':\n",
        "    all_mod_deco = list(set([x[:-1] for x in dataf.columns]))\n",
        "    for mod_deco in all_mod_deco:\n",
        "      #select all columns which have in common the column name apart from the final number (T5_tk, DIALO_tp, ecc)\n",
        "      filter_mod_deco = [col for col in data.columns if col.startswith(mod_deco)]\n",
        "      dataf_mod_deco = dataf[filter_mod_deco]\n",
        "      #create a column with names of columns with highest rouge score for that row\n",
        "      out_data['max_'+mod_deco] = dataf_mod_deco.idxmax(axis=1)\n",
        "      selected = []\n",
        "      scores = []\n",
        "      for i in range(len(data)):\n",
        "        c = out_data.loc[i, 'max_'+mod_deco]\n",
        "        selected.append(data.loc[i,c.replace(metric+'_','')])\n",
        "        scores.append(data.loc[i,c])\n",
        "      out_data[mod_deco+'_selected'] = selected\n",
        "      out_data[mod_deco+'_score'] = scores\n",
        "\n",
        "  #the highest generation is chosen for each model (e.g. highest T5, highest DIALO, ecc)\n",
        "  if by == 'model':\n",
        "    all_mod = list(set([x[:-4] for x in dataf.columns]))\n",
        "    for mod in all_mod:\n",
        "      #select all columns which have in common the column name apart from the final number (T5_tk, DIALO_tp, ecc)\n",
        "      filter_mod = [col for col in data.columns if col.startswith(mod)]\n",
        "      dataf_mod = dataf[filter_mod]\n",
        "      #create a column with names of columns with highest rouge score for that row\n",
        "      out_data['max_'+mod] = dataf_mod.idxmax(axis=1)\n",
        "      selected = []\n",
        "      scores = []\n",
        "      for i in range(len(data)):\n",
        "        c = out_data.loc[i, 'max_'+mod]\n",
        "        selected.append(data.loc[i,c.replace(metric+'_','')])\n",
        "        scores.append(data.loc[i,c])\n",
        "      out_data[mod+'_selected'] = selected\n",
        "    # data['decoding'] = data['rouge_max']\n",
        "      out_data[mod+'_score'] = scores\n",
        "\n",
        "    #the highest generation is chosen for each model (e.g. highest T5, highest DIALO, ecc)\n",
        "  if by == 'decoding':\n",
        "    all_deco = list(set([x[-3:-1] for x in dataf.columns]))\n",
        "    for deco in all_deco:\n",
        "      #select all columns which have in common the column name apart from the final number (T5_tk, DIALO_tp, ecc)\n",
        "      filter_deco = [col for col in data.columns if deco in col and col.startswith(metric)]\n",
        "      dataf_deco = dataf[filter_deco]\n",
        "      #create a column with names of columns with highest rouge score for that row\n",
        "      out_data['max_'+metric+'_'+deco] = dataf_deco.idxmax(axis=1)\n",
        "      selected = []\n",
        "      scores = []\n",
        "      for i in range(len(data)):\n",
        "        c = out_data.loc[i, 'max_'+metric+'_'+deco]\n",
        "        selected.append(data.loc[i,c.replace(metric+'_','')])\n",
        "        scores.append(data.loc[i,c])\n",
        "      out_data[metric+'_'+deco+'_selected'] = selected\n",
        "      out_data[metric+'_'+deco+'_score'] = scores\n",
        "\n",
        "  #select generation with highest score row-wise, amongst all generations\n",
        "  if by=='all':\n",
        "    out_data[metric+'_max'] = dataf.idxmax(axis=1)\n",
        "    selected = []\n",
        "    scores = []\n",
        "    for i in range(len(data)):\n",
        "      c = out_data.loc[i,metric+'_max']\n",
        "      selected.append(data.loc[i,c.replace(metric+'_','')])\n",
        "      scores.append(data.loc[i,c])\n",
        "    out_data['selected'] = selected\n",
        "    out_data[metric+'_all_score'] = scores\n",
        "\n",
        "  return out_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HlygLs3hmMi"
      },
      "outputs": [],
      "source": [
        "def tabler(selected_data, metric, digits = 5):\n",
        "  \"\"\"\n",
        "  input\n",
        "    selected_data: data evaluated by selecter\n",
        "    metric: a string, for the moment only rouge implemented\n",
        "\n",
        "  output\n",
        "    dataframe containing the results calculated on the selected columns\n",
        "  \"\"\"\n",
        "  data = selected_data.copy()\n",
        "  filter_col = [col for col in data.columns if col.startswith(metric) and col.endswith('score')]\n",
        "  diz = {'subset':[], 'min':[], 'max':[], 'mean':[], 'median':[], 'std':[]}\n",
        "  for c in filter_col:\n",
        "    diz['subset'].append(c.replace(metric+'_','').replace('_score',''))\n",
        "    diz['min'].append(round(data[c].min(),digits))\n",
        "    diz['max'].append(round(data[c].max(),digits))\n",
        "    diz['mean'].append(round(data[c].mean(),digits))\n",
        "    diz['median'].append(round(data[c].median(),digits))\n",
        "    diz['std'].append(round(data[c].std(),digits))\n",
        "  return pandas.DataFrame(diz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTGMt3gJmRz7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def tabler2(selected_data, digits = 5):\n",
        "  \"\"\"\n",
        "  input\n",
        "    selected_data: data evaluated by selecter\n",
        "    metric: a string, for the moment only rouge implemented\n",
        "\n",
        "  output\n",
        "    dataframe containing the results calculated on the selected columns\n",
        "  \"\"\"\n",
        "  data = selected_data.copy()\n",
        "  filter_col = [col for col in data.columns if re.match(\"[\\w\\-\\d]+_generated_CN\", col)]\n",
        "  diz = {'subset':[], 'min':[], 'max':[], 'mean':[], 'median':[], 'std':[]}\n",
        "  for c in filter_col:\n",
        "    diz['subset'].append(c.replace('_generated_CN', '_score'))\n",
        "    diz['min'].append(round(data[c].min(),digits))\n",
        "    diz['max'].append(round(data[c].max(),digits))\n",
        "    diz['mean'].append(round(data[c].mean(),digits))\n",
        "    diz['median'].append(round(data[c].median(),digits))\n",
        "    diz['std'].append(round(data[c].std(),digits))\n",
        "  return pandas.DataFrame(diz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aoOVLzxp0C1"
      },
      "outputs": [],
      "source": [
        "def tabler3(selected_data, metric, what_to_calculate, digits = 5):\n",
        "  \"\"\"\n",
        "  it can be visualised one thing at a time (what_to_calculate= mean, min, max, median, std)\n",
        "  and as cross-table of model and decoding\n",
        "  \"\"\"\n",
        "  data = selected_data.copy()\n",
        "  diz={'decoding':['tp','tk','kp','bs',],'T5':[0,0,0,0],'dialoGPT':[0,0,0,0],'gpt2':[0,0,0,0],'BART':[0,0,0,0],\n",
        "     'BERT':[0,0,0,0]}\n",
        "  d = pandas.DataFrame(diz)\n",
        "  for col in data.columns:\n",
        "    if col.startswith('max'):\n",
        "      subsets = list(set([x[:-1] for x in data[col]]))\n",
        "      for s in subsets:\n",
        "        model=s.split('_')[1]\n",
        "        deco = s.split('_')[2]\n",
        "        data_sub = data[data[col].str.contains(s)]\n",
        "        if what_to_calculate == 'mean':\n",
        "          d.loc[d['decoding']==deco,model]=round(data_sub[col.replace('max_','')+'_score'].mean(),digits)\n",
        "        if what_to_calculate == 'min':\n",
        "          d.loc[d['decoding']==deco,model]=round(data_sub[col.replace('max_','')+'_score'].min(),digits)\n",
        "        if what_to_calculate == 'max':\n",
        "          d.loc[d['decoding']==deco,model]=round(data_sub[col.replace('max_','')+'_score'].max(),digits)\n",
        "        if what_to_calculate == 'median':\n",
        "          d.loc[d['decoding']==deco,model]=round(data_sub[col.replace('max_','')+'_score'].median(),digits)\n",
        "        if what_to_calculate == 'std':\n",
        "          d.loc[d['decoding']==deco,model]=round(data_sub[col.replace('max_','')+'_score'].std(),digits)\n",
        "        if what_to_calculate == 'count':\n",
        "          diz[model]=len(data_sub)\n",
        "\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW144fhvKbi-"
      },
      "outputs": [],
      "source": [
        "def summaryr(collection_of_tables,\n",
        "                                  size_of_models = 'small',\n",
        "                                  include_median = False):\n",
        "\n",
        "  \"\"\"\n",
        "  input: a collection of 4 tables, created with tabler1\n",
        "  output: a \"stacked\" table with comparisons\n",
        "  important: follow this order 'ROUGE', 'BLEU-1', 'BLEU-3', 'BLEU-4'\n",
        "  \"\"\"\n",
        "\n",
        "  summary = [el.copy() for el in collection_of_tables] # careful: follow the order\n",
        "  metric_name = ('ROUGE', 'BLEU-1', 'BLEU-3', 'BLEU-4')\n",
        "  assert len(summary) == len(metric_name)\n",
        "\n",
        "  for i in range(len(summary)):\n",
        "    ctab = summary[i]\n",
        "    ctab['metric'] = metric_name[i]\n",
        "\n",
        "    # boolean column for winner\n",
        "    mmean = ctab['mean'].max()\n",
        "    ctab['highest_mean'] = False\n",
        "    ctab.loc[ctab['mean'] >= mmean, 'highest_mean'] = True\n",
        "\n",
        "    # int column for rank\n",
        "    ctab.sort_values(by='mean', ascending = False, inplace = True)\n",
        "    ctab['rank_mean'] = np.arange(1,len(ctab)+1)\n",
        "\n",
        "    if include_median:\n",
        "      # boolean column for median winner\n",
        "      mmean = ctab['median'].max()\n",
        "      ctab['highest_median'] = False\n",
        "      ctab.loc[ctab['median'] >= mmean, 'highest_median'] = True\n",
        "\n",
        "  done = pandas.concat(summary)\n",
        "  done = done.reset_index(drop=True)\n",
        "  done = done.replace({'gpt2': 'GPT-2', 'dialoGPT': 'DialoGPT'})\n",
        "  done = done.sort_values(by=['rank_mean', 'metric']).reset_index(drop = True)\n",
        "  done['size'] = size_of_models\n",
        "  done = done.rename(columns = {'subset':'model'})\n",
        "  done = done.reindex(columns=['size', 'model', 'metric', 'highest_mean', 'rank_mean', 'mean', 'std', 'min', 'median', 'max'])\n",
        "  return done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC91hC_Gd4xi"
      },
      "outputs": [],
      "source": [
        "def metrics_(collection_of_tables,\n",
        "                                  size_of_models = 'small'):\n",
        "\n",
        "  \"\"\"\n",
        "  input: a collection of 4 tables, created with tabler1\n",
        "  output: a \"stacked\" table with comparisons\n",
        "  important: follow this order 'ROUGE', 'BLEU-1', 'BLEU-3', 'BLEU-4'\n",
        "  \"\"\"\n",
        "\n",
        "  summary = [el.copy() for el in collection_of_tables] # careful: follow the order\n",
        "  metric_name = ('ROUGE', 'BLEU-1', 'BLEU-3', 'BLEU-4')\n",
        "  assert len(summary) == len(metric_name)\n",
        "\n",
        "  new = []\n",
        "  for i in range(len(summary)):\n",
        "    ctab = summary[i]\n",
        "    ctab = ctab.sort_values(by = 'subset').reset_index(drop=True)\n",
        "    met = metric_name[i]\n",
        "    ctab.columns = ['subset', 'min{}'.format(met), 'max{}'.format(met),\n",
        "                    'mean{}'.format(met), 'median{}'.format(met), 'std{}'.format(met)]\n",
        "\n",
        "    new.append(ctab)\n",
        "\n",
        "\n",
        "  from functools import reduce\n",
        "  df = reduce(lambda df1,df2: pandas.merge(df1,df2,on='subset'), new)\n",
        "\n",
        "  df['model']  = df.subset.apply(lambda x: x.split('_')[0])\n",
        "  df['decoding']  = df.subset.apply(lambda x: x.split('_')[1])\n",
        "  df['size'] = size_of_models\n",
        "\n",
        "  df = df.reindex(columns = ['size', 'model', 'decoding', 'minROUGE', 'maxROUGE', 'meanROUGE', 'medianROUGE',\n",
        "       'stdROUGE', 'minBLEU-1', 'maxBLEU-1', 'meanBLEU-1', 'medianBLEU-1',\n",
        "       'stdBLEU-1', 'minBLEU-3', 'maxBLEU-3', 'meanBLEU-3', 'medianBLEU-3',\n",
        "       'stdBLEU-3', 'minBLEU-4', 'maxBLEU-4', 'meanBLEU-4', 'medianBLEU-4',\n",
        "       'stdBLEU-4'])\n",
        "\n",
        "  df = df.replace({'gpt2': 'GPT-2', 'dialoGPT': 'DialoGPT'})\n",
        "\n",
        "  df = df.sort_values(by='meanROUGE', ascending=False).reset_index(drop=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def displayres(master, what ='mean'):\n",
        "\n",
        "  \"\"\"this takes the result of metrics_ and display it\"\"\"\n",
        "\n",
        "  if what == 'mean':\n",
        "    c = '#e28413'\n",
        "  if what == 'max':\n",
        "    c = '#de3c4b'\n",
        "\n",
        "  for mod in master.model.unique():\n",
        "    curr = master[master.model == mod].copy()\n",
        "    d_col = [el for el in master.columns if what in el]\n",
        "    print('{} ({})'.format(curr['model'].unique()[0], curr['size'].unique()[0]))\n",
        "    curr = curr[[\n",
        "                #  'size', 'model',\n",
        "                 'decoding',\n",
        "                 '{}ROUGE'.format(what), '{}BLEU-1'.format(what),\n",
        "                 '{}BLEU-3'.format(what), '{}BLEU-4'.format(what)\n",
        "                 ]]\n",
        "    curr = curr.sort_values(by='decoding').reset_index(drop=True)\n",
        "    display(curr.style.highlight_max(d_col, color = c, axis = 0))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Uii9C5MsB3"
      },
      "source": [
        "## Compute scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiRKVXVWQdsV"
      },
      "outputs": [],
      "source": [
        "# path = path to_generated_data\n",
        "\n",
        "generations = pandas.read_csv('drive/MyDrive/master/output_.csv', delimiter=';')\n",
        "\n",
        "generations.head(1)\n",
        "# len(generations.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4idanHQQdFA"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "df_eval = evaluator(somedata=generations, metric='rouge')\n",
        "df_eval_bsingle = evaluator(somedata=generations, metric='bleu-1')\n",
        "#df_eval_b3 = evaluator(somedata=generations, metric='bleu-3')\n",
        "#df_eval_b4 = evaluator(somedata=generations, metric='bleu-4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRpK3EL5hxnv"
      },
      "outputs": [],
      "source": [
        "df_eval_merged = reduce(lambda left,right: pandas.merge(left,right,on=['index', 'HS', 'generated_CN', 'reference_CN', 'Decoding'],\n",
        "                                            how='outer'), [df_eval, df_eval_bsingle])\n",
        "df_eval_merged.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0Oz_bCtOS6t"
      },
      "outputs": [],
      "source": [
        "df_eval_merged.to_csv('drive/MyDrive/master/metrics_output_gpt3.5.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0KJLgr1knTy"
      },
      "outputs": [],
      "source": [
        "tabler2(df_eval_merged) #over all decodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiH1d740pVOR"
      },
      "outputs": [],
      "source": [
        "tabler2(df_eval_merged[df_eval_merged['Decoding']=='beam-search'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_j6XknnpdXl"
      },
      "outputs": [],
      "source": [
        "tabler2(df_eval_merged[df_eval_merged['decoding']=='top-p'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOV9iMvCqE7D"
      },
      "outputs": [],
      "source": [
        "tabler2(df_eval_merged[df_eval_merged['decoding']=='top-k'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDjiSy0zqFbX"
      },
      "outputs": [],
      "source": [
        "tabler2(df_eval_merged[df_eval_merged['decoding']=='k-p'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viHc-IhG3Fj8"
      },
      "outputs": [],
      "source": [
        "# filename = 'TEST_LARGE_5seq_rouge.csv'\n",
        "# you_want_to_save = True\n",
        "# if you_want_to_save:\n",
        "#   df_eval.to_csv(filename, index = False, encoding = 'utf-8')\n",
        "#   from google.colab import files\n",
        "#   files.download(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKdPKOOl32uW"
      },
      "outputs": [],
      "source": [
        "# filename = 'TEST_LARGE_5seq_bleu-1.csv'\n",
        "# you_want_to_save = True\n",
        "# if you_want_to_save:\n",
        "#   df_eval_bsingle.to_csv(filename, index = False, encoding = 'utf-8')\n",
        "#   from google.colab import files\n",
        "#   files.download(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-7IG1s4AGPL"
      },
      "outputs": [],
      "source": [
        "# filename = 'TEST_LARGE_5seq_bleu-3.csv'\n",
        "# you_want_to_save = True\n",
        "# if you_want_to_save:\n",
        "#   df_eval_b3.to_csv(filename, index = False, encoding = 'utf-8')\n",
        "#   from google.colab import files\n",
        "#   files.download(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpL-o9oSB2yx"
      },
      "outputs": [],
      "source": [
        "# filename = 'TEST_LARGE_5seq_bleu-4.csv'\n",
        "# you_want_to_save = True\n",
        "# if you_want_to_save:\n",
        "#   df_eval_b4.to_csv(filename, index = False, encoding = 'utf-8')\n",
        "#   from google.colab import files\n",
        "#   files.download(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW5P4MEzv13e"
      },
      "source": [
        "## Select best CN among all decoding mecanisms for each model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beL336593T08"
      },
      "source": [
        "#### Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuYbvTNLz4Eh"
      },
      "outputs": [],
      "source": [
        "df_eval = pandas.read_csv('drive/MyDrive/master/trial_set_metrics.csv', delimiter=';', index_col='index')\n",
        "df_sel = selecter(evaluated_data= df_eval, metric='rouge', by='model_decoding')\n",
        "df_tab = tabler(selected_data= df_sel, metric='rouge')\n",
        "df_tab.style.highlight_max(['max', 'mean'], color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvizF6JPxbbv"
      },
      "outputs": [],
      "source": [
        "df_tab3 = tabler3(selected_data= df_sel, metric='rouge', what_to_calculate='mean')\n",
        "df_tab3.style.highlight_max(color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0EjQBFK3X_y"
      },
      "source": [
        "#### Bleu-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acsRKxFu3eZ8"
      },
      "outputs": [],
      "source": [
        "df_eval_bsingle = pandas.read_csv('TEST_LARGE_5seq_bleu-1.csv')\n",
        "df_sel_bsingle = selecter(evaluated_data= df_eval_bsingle, metric='bleu-1', by='model')\n",
        "df_tab_bsingle = tabler(selected_data= df_sel_bsingle, metric='bleu-1')\n",
        "df_tab_bsingle.style.highlight_max(['max', 'mean'], color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cLtmL0YWHfb"
      },
      "outputs": [],
      "source": [
        "df_tab3_bsingle = tabler3(selected_data= df_sel_bsingle, metric='bleu-1', what_to_calculate='mean')\n",
        "df_tab3_bsingle.style.highlight_max(color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLJmo2-2AGPI"
      },
      "source": [
        "#### Bleu-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gO6eLwWAGPJ"
      },
      "outputs": [],
      "source": [
        "df_eval_b3 = pandas.read_csv('TEST_LARGE_5seq_bleu-3.csv')\n",
        "df_sel_b3 = selecter(evaluated_data= df_eval_b3, metric='bleu-3', by='model')\n",
        "df_tab_b3 = tabler(selected_data= df_sel_b3, metric='bleu-3')\n",
        "df_tab_b3.style.highlight_max(['max', 'mean'], color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N52ZfiE2AGPK"
      },
      "outputs": [],
      "source": [
        "df_tab3_b3 = tabler3(selected_data= df_sel_b3, metric='bleu-3', what_to_calculate='mean')\n",
        "df_tab3_b3.style.highlight_max(color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEF7ixE_B2yt"
      },
      "source": [
        "#### Bleu-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yky3rKjIB2yu"
      },
      "outputs": [],
      "source": [
        "df_eval_b4 = pandas.read_csv('TEST_LARGE_5seq_bleu-4.csv')\n",
        "df_sel_b4 = selecter(evaluated_data= df_eval_b4, metric='bleu-4', by='model')\n",
        "df_tab_b4 = tabler(selected_data= df_sel_b4, metric='bleu-4')\n",
        "df_tab_b4.style.highlight_max(['max', 'mean'], color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csr5c-0BB2yw"
      },
      "outputs": [],
      "source": [
        "df_tab3_b4 = tabler3(selected_data= df_sel_b4, metric='bleu-4', what_to_calculate='mean')\n",
        "df_tab3_b4.style.highlight_max(color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGY-9ne1TQs2"
      },
      "source": [
        "### *🡲 summary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu91aIg3Tlv8"
      },
      "outputs": [],
      "source": [
        "avgl = summaryr((df_tab, df_tab_bsingle, df_tab_b3, df_tab_b4), size_of_models='large')\n",
        "# avgl.to_csv('eval_summary1_large.csv', index = False)\n",
        "avgl.style.highlight_max(['max', 'median', 'mean'], color = c, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYiPSChgTU8m"
      },
      "outputs": [],
      "source": [
        "avgl[avgl.highest_mean].round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CETdypb0fNjM"
      },
      "source": [
        "## Select best CN for each ***combination*** decoding mecanism-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaZ7SqFnL6ys"
      },
      "source": [
        "#### Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGOh-24l_rs6"
      },
      "outputs": [],
      "source": [
        "df_eval = pandas.read_csv('TEST_LARGE_5seq_rouge.csv')\n",
        "df_sel_md = selecter(evaluated_data= df_eval, metric='rouge', by='model_decoding')\n",
        "df_tab_md = tabler(selected_data= df_sel_md, metric='rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Si8lwaWAcRe"
      },
      "outputs": [],
      "source": [
        "df_sel_md = selecter(evaluated_data= df_eval, metric='rouge', by='model_decoding')\n",
        "df_tab_md = tabler3(selected_data= df_sel_md, metric='rouge', what_to_calculate='mean')\n",
        "df_tab_md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckqHJRkBB3Pw"
      },
      "source": [
        "#### Bleu-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFiC0NtxB3Py"
      },
      "outputs": [],
      "source": [
        "df_eval_lg_bsingle= pandas.read_csv('TEST_LARGE_5seq_bleu-1.csv')\n",
        "df_sel_lg_md_bsingle = selecter(evaluated_data= df_eval_lg_bsingle, metric='bleu-1', by='model_decoding')\n",
        "df_tab_lg_md2_bsingle = tabler3(selected_data= df_sel_lg_md_bsingle, metric='bleu-1', what_to_calculate='mean')\n",
        "df_tab_lg_md2_bsingle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iLW0GtRB3Pz"
      },
      "source": [
        "#### Bleu-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awz8l4rJB3P0"
      },
      "outputs": [],
      "source": [
        "df_eval_lg_b3= pandas.read_csv('TEST_LARGE_5seq_bleu-3.csv')\n",
        "df_sel_lg_md_b3 = selecter(evaluated_data= df_eval_lg_b3, metric='bleu-3', by='model_decoding')\n",
        "df_tab_lg_md2_b3 = tabler3(selected_data= df_sel_lg_md_b3, metric='bleu-3', what_to_calculate='mean')\n",
        "df_tab_lg_md2_b3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BPIqDp5B3P1"
      },
      "source": [
        "#### Bleu-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GPMpOnBB3P2"
      },
      "outputs": [],
      "source": [
        "df_eval_lg_b4= pandas.read_csv('TEST_LARGE_5seq_bleu-4.csv')\n",
        "df_sel_lg_md_b4 = selecter(evaluated_data= df_eval_lg_b4, metric='bleu-4', by='model_decoding')\n",
        "df_tab_lg_md2_b4 = tabler3(selected_data= df_sel_lg_md_b4, metric='bleu-4', what_to_calculate='mean')\n",
        "df_tab_lg_md2_b4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39sqErfWc1u_"
      },
      "source": [
        "### *🡲 summary*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUI6JpEsJ4rh"
      },
      "outputs": [],
      "source": [
        "df_tab_lg_md = tabler(selected_data= df_sel_md, metric='rouge')\n",
        "df_tab_lg_md_bsingle = tabler(selected_data= df_sel_lg_md_bsingle, metric='bleu-1')\n",
        "df_tab_lg_md_b3 = tabler(selected_data= df_sel_lg_md_b3, metric='bleu-3')\n",
        "df_tab_lg_md_b4 = tabler(selected_data= df_sel_lg_md_b4, metric='bleu-4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXj15Dofc2Bw"
      },
      "outputs": [],
      "source": [
        "tabs = [df_tab_lg_md, df_tab_lg_md_bsingle, df_tab_lg_md_b3, df_tab_lg_md_b4]\n",
        "evall = metrics_(tabs, size_of_models='large')\n",
        "# evall.to_csv('evaluation_large_models.csv', index=False)\n",
        "evall.style.highlight_max([el for el in evall.columns if 'mean' in el], color = c, axis = 0).highlight_max([el for el in evall.columns if 'max' in el], color = 'yellow', axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2RcZi0CdZAb"
      },
      "outputs": [],
      "source": [
        "displayres(evall, 'mean')\n",
        "displayres(evall, 'max')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6jzzTD8WI_K"
      },
      "source": [
        "# [syntactic complexity](https://spacy.io/usage/linguistic-features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMlWPhPvYgGM"
      },
      "source": [
        "- Maximum Syntactic Depth (MSD): the maximum depth among the dependency trees calculated over each sentence composing a CN.\n",
        "- Average Syntactic Depth (ASD): the depth of the sentences in each CN.\n",
        "- Number of Sentences (NST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZNM21w3b1mH"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import statistics\n",
        "\n",
        "en_nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKqdb_f7WFLj"
      },
      "outputs": [],
      "source": [
        "def walk_tree(node, depth):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return max(walk_tree(child, depth + 1) for child in node.children)\n",
        "    else:\n",
        "        return depth\n",
        "\n",
        "def get_max_sd(data):\n",
        "  docu = en_nlp(data['text'])\n",
        "  return max([walk_tree(sent.root, 0) for sent in docu.sents])\n",
        "def get_avg_sd(data):\n",
        "  docu = en_nlp(data['text'])\n",
        "  return statistics.mean([walk_tree(sent.root, 0) for sent in docu.sents])\n",
        "def get_nst(data):\n",
        "  docu = en_nlp(data['text'])\n",
        "  return len([sent for sent in docu.sents])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nwz_TZ_cPlF"
      },
      "outputs": [],
      "source": [
        "df['msd'] = df.apply(get_max_sd, axis=1)\n",
        "df['asd'] = df.apply(get_avg_sd, axis=1)\n",
        "df['nst'] = df.apply(get_nst, axis=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b0EjQBFK3X_y",
        "mLJmo2-2AGPI",
        "OEF7ixE_B2yt"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}