{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXENj6bAiMan"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhzihhddiMaq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pandas\n",
        "import time\n",
        "import random\n",
        "from collections import Counter\n",
        "from statistics import mean, median, stdev\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#df = pandas.read_csv('drive/MyDrive/master/new_multi_conan.csv')\n",
        "val_df = pandas.read_csv('drive/MyDrive/master/testset_multi_conan.csv', sep=',')\n",
        "\n",
        "# #156 is the longest HS-CN text, and it will be used as max_len for both encoder and decoder\n",
        "max_len=156\n",
        "batch_size=20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvuxf3IwzBD3"
      },
      "source": [
        "###Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxzgiXiHiMap"
      },
      "outputs": [],
      "source": [
        "decoder_max_len = max_len\n",
        "encoder_max_len = max_len\n",
        "\n",
        "# !pip install transformers==4.0.0\n",
        "!pip install torch==1.6.0\n",
        "\n",
        "!pip install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets==1.0.2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import gc\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "is_cloud = True\n",
        "model_name = 'gpt2'\n",
        "if is_cloud:\n",
        "    model_name = 'gpt2-large'\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75ka4guf_ril"
      },
      "outputs": [],
      "source": [
        "class HSCNDataset2(Dataset):\n",
        "    def __init__(self, df, add_special_tokens=False):\n",
        "      #the super() builtin returns a proxy object, a substitute object that can call methods of the base class (Dataset) via delegation\n",
        "        super().__init__()\n",
        "\n",
        "        self.hscn_list = []\n",
        "\n",
        "        if add_special_tokens:\n",
        "            self.start_of_hs_token = \"<hatespeech>\"\n",
        "            self.end_of_hs_token = \"<counternarrative>\"\n",
        "        else:\n",
        "            self.start_of_hs_token = \"<|hatespeech|>\"\n",
        "            self.end_of_hs_token = \"<|counternarrative|>\"\n",
        "\n",
        "\n",
        "        #This is the end of text token originally used in GPT2\n",
        "        # self.end_of_text_token = \"<|endoftext|>\"\n",
        "\n",
        "        x = 0\n",
        "        for _, row in df.iterrows():\n",
        "          #\n",
        "          hscn = f\"{self.start_of_hs_token} {row['HATE_SPEECH']} {self.end_of_hs_token}\"\n",
        "          self.hscn_list.append(hscn)\n",
        "\n",
        "        # random.shuffle(self.hscn_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hscn_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.hscn_list[item]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvRoJu3D-gcB"
      },
      "outputs": [],
      "source": [
        "gc.collect() #this is useful in order not to load cuda\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#given that we can't map the dataset, we generate by batches through the Pytorch's DataLoader class\n",
        "def generate_cn(model, hscndata, output_data, decoding, p=0.92, k=40, nb=1, rp=2.0, specialtokens=True):\n",
        "    gc.collect() #this is useful in order not to load cuda\n",
        "    torch.cuda.empty_cache()\n",
        "    output_df = output_data.copy()\n",
        "    print(output_df.head())\n",
        "    print(len(output_df))\n",
        "    print(len(hscndata))\n",
        "    new_cns=[]\n",
        "    outs = []\n",
        "    for i in range(len(hscndata)):\n",
        "\n",
        "        encoded_hs_ids = tokenizer(hscndata[i],\n",
        "                                truncation=True,\n",
        "                                padding=True,\n",
        "                                return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "        if decoding == 'top-p':\n",
        "\n",
        "            name_col='tp'\n",
        "            encoded_new_cn = model.generate(input_ids = encoded_hs_ids,\n",
        "                                      max_length=decoder_max_len,\n",
        "                                      do_sample=True,\n",
        "                                      top_p=p,\n",
        "                                      num_return_sequences = nb)\n",
        "            new_cns.extend(tokenizer.batch_decode(encoded_new_cn, skip_special_tokens=False))\n",
        "            clear_output()\n",
        "            #print(decoding)\n",
        "            #print(len(new_cns))\n",
        "\n",
        "        if decoding == 'top-k':\n",
        "            name_col='tk'\n",
        "            encoded_new_cn = model.generate(input_ids = encoded_hs_ids,\n",
        "                                      max_length = decoder_max_len,\n",
        "                                      do_sample = True,\n",
        "                                      top_k = k,\n",
        "                                      num_return_sequences = nb)\n",
        "            new_cns.extend(tokenizer.batch_decode(encoded_new_cn, skip_special_tokens=False))\n",
        "            clear_output()\n",
        "            # print(decoding)\n",
        "            # print(len(new_cns))\n",
        "\n",
        "        if decoding == 'beam-search':\n",
        "            name_col='bs'\n",
        "            encoded_new_cn = model.generate(input_ids = encoded_hs_ids,\n",
        "                                      max_length=decoder_max_len,\n",
        "                                      num_beams=nb,\n",
        "                                      early_stopping=True,\n",
        "                                      num_return_sequences=nb,\n",
        "                                      repetition_penalty=rp,\n",
        "                                      do_sample=True)\n",
        "            new_cns.extend(tokenizer.batch_decode(encoded_new_cn, skip_special_tokens=False))\n",
        "            #clear_output()\n",
        "            print(decoding)\n",
        "            print(len(new_cns))\n",
        "            #print(new_cns)\n",
        "\n",
        "        if decoding == 'k-p':\n",
        "            name_col='kp'\n",
        "            encoded_new_cn = model.generate(input_ids = encoded_hs_ids,\n",
        "                                      max_length = decoder_max_len,\n",
        "                                      do_sample = True,\n",
        "                                      top_k = k,\n",
        "                                      top_p = p,\n",
        "                                      num_return_sequences = nb)\n",
        "            new_cns.extend(tokenizer.batch_decode(encoded_new_cn, skip_special_tokens=False))\n",
        "            clear_output()\n",
        "           # print(decoding)\n",
        "           # print(len(new_cns))\n",
        "\n",
        "        #for j in range(len(new_cns)):\n",
        "         #   c = new_cns[j].split('<counternarrative>')\n",
        "          #  s = c[1].lstrip().strip(' <|endoftext|>')\n",
        "        #if pandas.isnull(s):\n",
        "         #   outs.append('')\n",
        "        #else:\n",
        "         #   outs.append(s)\n",
        "    #print(outs)\n",
        "    #chunks=[outs[i:i+5] for i in range(0,len(outs),5)]\n",
        "    #print(chunks)\n",
        "    #for i in range(5):\n",
        "     #   toappend= [chunk[i] for chunk in chunks]\n",
        "      #  print(toappend)\n",
        "       # output_df[name_col+str(i)] = toappend\n",
        "\n",
        "    clear_output()\n",
        "    print(new_cns)\n",
        "    return new_cns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlals634zGL5"
      },
      "source": [
        "###Load model and prepare data\n",
        "(`add_special_tokens = True`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX97I8JqveVH"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "model_path = 'drive/MyDrive/master/checkpoints/'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path, return_tensors = 'pt')\n",
        "model1 = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "model1 = model1.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjYqT870zI-C"
      },
      "outputs": [],
      "source": [
        "# this is necessary in order to pad\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # config.pad_token_id = config.eos_token_id\n",
        "print(tokenizer.special_tokens_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt6Pe0Vy1c6O"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "v_df = val_df[['HATE_SPEECH']]#, 'COUNTER_NARRATIVE', 'TARGET','VERSION']]\n",
        "val_dataset1 = HSCNDataset2(v_df,add_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E07cLEaezQxT"
      },
      "source": [
        "### Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8EzJAZXzlAI"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLFRtCdNPEmr"
      },
      "outputs": [],
      "source": [
        "#%%time\n",
        "you_want_to_save = True\n",
        "v_df1 = v_df.copy()\n",
        "\n",
        "seed=42\n",
        "torch.manual_seed(seed)\n",
        "decodings = ['beam-search']\n",
        "list_dfs = []\n",
        "hscnpairs = pandas.DataFrame(columns=[\"HS\", \"CN\", \"Decoding\"])\n",
        "for k in decodings:\n",
        "    fresh_cn =generate_cn(model=model1, hscndata=val_dataset1, output_data=v_df1, decoding= k)\n",
        "    for i in range(len(fresh_cn)):\n",
        "      c = fresh_cn[i].split('<counternarrative>')\n",
        "      s = c[1].lstrip().strip(' <|endoftext|>')\n",
        "      hscnpairs.loc[len(hscnpairs)] = {\"HS\": v_df1.loc[i, [\"HATE_SPEECH\"]].item(), \"CN\": s, \"Decoding\": k}\n",
        "    #pandas_columns = [pandas.Series(fresh_cn[col], name = str(col)).reset_index(drop = True) for col in fresh_cn.columns]\n",
        "    #generated_data = pandas.concat([pcol for pcol in pandas_columns], axis = 1)\n",
        "    #list_dfs.append(generated_data)\n",
        "\n",
        "#merged = reduce(lambda  left,right: pandas.merge(left,right,on=['HATE_SPEECH'], how='outer'), list_dfs)\n",
        "#temp=sorted(list(merged.columns))\n",
        "#temp.remove('index')\n",
        "#temp.insert(0,'index')\n",
        "#merged=merged.reindex(columns=temp)\n",
        "hscnpairs.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hscnpairs.to_csv(\"drive/MyDrive/master/output_testset_multi_conan.csv\")"
      ],
      "metadata": {
        "id": "hzjjUfxh1o5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END OF NOTEBOOK**"
      ],
      "metadata": {
        "id": "UKRhLPNp7T5p"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}